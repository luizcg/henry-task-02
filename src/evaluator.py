"""
RAG Evaluator Agent - Quality Assessment for RAG Responses
Evaluates answer quality based on relevance, accuracy, completeness, and groundedness.
"""

import os
import warnings
from typing import Dict, Any, List
from dotenv import load_dotenv

# Suppress Pydantic v1 compatibility warnings for Python 3.14+
warnings.filterwarnings("ignore", message=".*Pydantic V1 functionality.*")

from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import HumanMessage, SystemMessage


class RAGEvaluator:
    """
    Evaluates RAG system responses for quality assurance.
    Scores answers 0-10 based on multiple criteria.
    """

    def __init__(
        self,
        llm_model: str = "gpt-4o",
        temperature: float = 0.1,
    ):
        """
        Initialize RAG evaluator.

        Args:
            llm_model: OpenAI model for evaluation
            temperature: Low temperature for consistent evaluation
        """
        self.llm_model = llm_model
        self.llm = ChatOpenAI(model=llm_model, temperature=temperature)

        # Evaluation criteria with weights
        self.criteria = {
            "relevance": {
                "weight": 0.30,
                "description": "Does the answer directly address the question?",
            },
            "accuracy": {
                "weight": 0.25,
                "description": "Is the answer factually correct based on context?",
            },
            "completeness": {
                "weight": 0.20,
                "description": "Does the answer cover all aspects of the question?",
            },
            "groundedness": {
                "weight": 0.15,
                "description": "Is the answer grounded in the provided context chunks?",
            },
            "clarity": {
                "weight": 0.10,
                "description": "Is the answer clear, concise, and well-structured?",
            },
        }

        # System prompt for evaluation
        self.system_prompt = """You are an expert evaluator for Retrieval-Augmented Generation (RAG) systems.

Your task is to evaluate the quality of answers generated by a RAG system based on:
1. The user's question
2. The system's answer
3. The context chunks retrieved from the knowledge base

Evaluation Criteria (0-10 scale for each):

1. **Relevance (30%)**: Does the answer directly address the user's question?
   - 10: Perfect alignment with question
   - 7-9: Mostly relevant, minor tangents
   - 4-6: Partially relevant
   - 0-3: Off-topic or misunderstood question

2. **Accuracy (25%)**: Is the answer factually correct based on the provided context?
   - 10: Fully accurate, no errors
   - 7-9: Mostly accurate, minor imprecisions
   - 4-6: Some inaccuracies present
   - 0-3: Major factual errors or contradictions

3. **Completeness (20%)**: Does the answer cover all aspects of the question?
   - 10: Comprehensive, nothing missing
   - 7-9: Covers main points, minor gaps
   - 4-6: Significant information missing
   - 0-3: Incomplete or superficial

4. **Groundedness (15%)**: Is the answer grounded in the provided context chunks?
   - 10: Fully supported by context, no hallucinations
   - 7-9: Mostly grounded, minor extrapolations
   - 4-6: Some unsupported claims
   - 0-3: Major hallucinations or fabrications

5. **Clarity (10%)**: Is the answer clear, concise, and well-structured?
   - 10: Excellent clarity and structure
   - 7-9: Clear and understandable
   - 4-6: Somewhat confusing or verbose
   - 0-3: Unclear or poorly structured

Provide your evaluation as JSON with:
- Individual scores for each criterion (0-10)
- Overall weighted score (0-10)
- Detailed reasoning for the evaluation
- Specific strengths and weaknesses
- Suggestions for improvement"""

    def evaluate(
        self,
        user_question: str,
        system_answer: str,
        chunks_related: List[Dict[str, Any]],
    ) -> Dict[str, Any]:
        """
        Evaluate a RAG response.

        Args:
            user_question: The user's original question
            system_answer: The system's generated answer
            chunks_related: Retrieved context chunks with metadata

        Returns:
            Evaluation dict with score, reasoning, and breakdown
        """
        # Build evaluation prompt
        context_summary = self._format_chunks(chunks_related)

        evaluation_prompt = f"""
Please evaluate this RAG system response:

**USER QUESTION:**
{user_question}

**SYSTEM ANSWER:**
{system_answer}

**RETRIEVED CONTEXT CHUNKS:**
{context_summary}

**EVALUATION TASK:**
Score the answer on each criterion (0-10) and provide detailed reasoning.
Consider the relevance scores of the chunks when evaluating groundedness.

Return a JSON object with this exact structure:
{{
    "scores": {{
        "relevance": <0-10>,
        "accuracy": <0-10>,
        "completeness": <0-10>,
        "groundedness": <0-10>,
        "clarity": <0-10>
    }},
    "overall_score": <weighted average 0-10>,
    "reasoning": "Detailed explanation of the evaluation...",
    "strengths": ["strength 1", "strength 2", ...],
    "weaknesses": ["weakness 1", "weakness 2", ...],
    "suggestions": ["suggestion 1", "suggestion 2", ...]
}}
"""

        # Get evaluation from LLM
        messages = [
            SystemMessage(content=self.system_prompt),
            HumanMessage(content=evaluation_prompt),
        ]

        try:
            response = self.llm.invoke(messages)
            evaluation_text = response.content

            # Parse JSON response
            import json
            import re

            # Extract JSON from markdown code blocks if present
            json_match = re.search(r"```(?:json)?\s*(\{.*?\})\s*```", evaluation_text, re.DOTALL)
            if json_match:
                evaluation_text = json_match.group(1)

            evaluation = json.loads(evaluation_text)

            # Calculate overall score if not provided
            if "overall_score" not in evaluation or evaluation["overall_score"] is None:
                evaluation["overall_score"] = self._calculate_weighted_score(
                    evaluation["scores"]
                )

            # Add metadata
            evaluation["metadata"] = {
                "question": user_question,
                "num_chunks": len(chunks_related),
                "evaluator_model": self.llm_model,
                "evaluation_criteria": self.criteria,
            }

            return evaluation

        except Exception as e:
            # Fallback evaluation if parsing fails
            return {
                "scores": {
                    "relevance": 5,
                    "accuracy": 5,
                    "completeness": 5,
                    "groundedness": 5,
                    "clarity": 5,
                },
                "overall_score": 5.0,
                "reasoning": f"Evaluation failed: {str(e)}. Manual review recommended.",
                "strengths": ["Unable to evaluate automatically"],
                "weaknesses": ["Evaluation parsing error"],
                "suggestions": ["Review response manually"],
                "metadata": {
                    "error": str(e),
                    "question": user_question,
                    "num_chunks": len(chunks_related),
                    "evaluator_model": self.llm_model,
                },
            }

    def _format_chunks(self, chunks: List[Dict[str, Any]]) -> str:
        """Format chunks for evaluation prompt."""
        formatted = []
        for i, chunk in enumerate(chunks, 1):
            text = chunk.get("text", "")[:300]  # Truncate for brevity
            score = chunk.get("relevance_score", 0)
            formatted.append(f"[Chunk {i}] (Relevance: {score:.3f})\n{text}...")

        return "\n\n".join(formatted)

    def _calculate_weighted_score(self, scores: Dict[str, float]) -> float:
        """Calculate weighted overall score."""
        total = 0.0
        for criterion, score in scores.items():
            weight = self.criteria.get(criterion, {}).get("weight", 0)
            total += score * weight
        return round(total, 2)

    def evaluate_batch(
        self, responses: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Evaluate multiple RAG responses.

        Args:
            responses: List of response dicts with user_question, system_answer, chunks_related

        Returns:
            List of evaluation results
        """
        evaluations = []
        for i, response in enumerate(responses, 1):
            print(f"Evaluating response {i}/{len(responses)}...")
            
            evaluation = self.evaluate(
                user_question=response["user_question"],
                system_answer=response["system_answer"],
                chunks_related=response["chunks_related"],
            )
            
            evaluations.append(evaluation)

        return evaluations

    def get_summary_statistics(
        self, evaluations: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Calculate summary statistics across multiple evaluations.

        Args:
            evaluations: List of evaluation results

        Returns:
            Summary statistics
        """
        if not evaluations:
            return {}

        # Aggregate scores
        all_scores = {criterion: [] for criterion in self.criteria.keys()}
        overall_scores = []

        for eval_result in evaluations:
            overall_scores.append(eval_result["overall_score"])
            for criterion, score in eval_result["scores"].items():
                all_scores[criterion].append(score)

        # Calculate statistics
        def stats(values):
            return {
                "mean": round(sum(values) / len(values), 2),
                "min": round(min(values), 2),
                "max": round(max(values), 2),
            }

        return {
            "num_evaluations": len(evaluations),
            "overall": stats(overall_scores),
            "by_criterion": {
                criterion: stats(scores)
                for criterion, scores in all_scores.items()
            },
        }


def main():
    """Main entry point - accepts JSON from stdin or runs example."""
    import sys
    import argparse
    import json
    
    # Parse arguments
    parser = argparse.ArgumentParser(
        description="Evaluate RAG responses. Can read from stdin or use example.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Pipe from query.py
  python src/query.py "What is CUDA?" | python src/evaluator.py
  
  # Pipe with pretty output
  python src/query.py "What is CUDA?" | python src/evaluator.py --pretty
  
  # Output only JSON
  python src/query.py "What is CUDA?" | python src/evaluator.py --json
  
  # Run example (no stdin)
  python src/evaluator.py
        """
    )
    parser.add_argument("--json", action="store_true", help="Output only JSON (no formatting)")
    parser.add_argument("--pretty", action="store_true", help="Pretty print output")
    parser.add_argument("--score-only", action="store_true", help="Output only the score")
    args = parser.parse_args()
    
    load_dotenv()
    evaluator = RAGEvaluator()
    
    # Check if there's data in stdin
    if not sys.stdin.isatty():
        # Read from stdin
        try:
            stdin_data = sys.stdin.read().strip()
            response = json.loads(stdin_data)
            
            # Evaluate
            evaluation = evaluator.evaluate(
                user_question=response["user_question"],
                system_answer=response["system_answer"],
                chunks_related=response["chunks_related"],
            )
            
            # Add evaluation to response
            response["evaluation"] = evaluation
            
            # Output based on flags
            if args.score_only:
                print(evaluation["overall_score"])
            elif args.json:
                indent = 2 if args.pretty else None
                print(json.dumps(response, indent=indent, ensure_ascii=False))
            else:
                # Human-readable format
                print("=" * 80)
                print("üîç RAG Response Evaluation")
                print("=" * 80)
                print(f"\n‚ùì Question: {response['user_question']}")
                print(f"\nüìä Overall Score: {evaluation['overall_score']}/10")
                print(f"\nüìã Breakdown:")
                for criterion, score in evaluation["scores"].items():
                    weight = evaluator.criteria[criterion]["weight"]
                    print(f"  - {criterion.capitalize()}: {score}/10 (weight: {weight*100}%)")
                
                print(f"\nüí≠ Reasoning:\n{evaluation['reasoning']}")
                
                if evaluation.get("strengths"):
                    print(f"\n‚úÖ Strengths:")
                    for strength in evaluation["strengths"]:
                        print(f"  ‚Ä¢ {strength}")
                
                if evaluation.get("weaknesses"):
                    print(f"\n‚ö†Ô∏è  Weaknesses:")
                    for weakness in evaluation["weaknesses"]:
                        print(f"  ‚Ä¢ {weakness}")
                
                if evaluation.get("suggestions"):
                    print(f"\nüí° Suggestions:")
                    for suggestion in evaluation["suggestions"]:
                        print(f"  ‚Ä¢ {suggestion}")
                
                print("=" * 80)
                
        except json.JSONDecodeError as e:
            print(f"‚ùå Error: Invalid JSON input from stdin", file=sys.stderr)
            print(f"   {e}", file=sys.stderr)
            sys.exit(1)
        except KeyError as e:
            print(f"‚ùå Error: Missing required field in JSON: {e}", file=sys.stderr)
            sys.exit(1)
        except Exception as e:
            print(f"‚ùå Error: {e}", file=sys.stderr)
            sys.exit(1)
    
    else:
        # No stdin - run example
        sample_response = {
            "user_question": "What is NVIDIA's CUDA programming model?",
            "system_answer": "NVIDIA's CUDA programming model is a parallel computing platform introduced in 2006 that enables developers to use GPUs for general-purpose computing beyond graphics rendering.",
            "chunks_related": [
                {
                    "text": "Q5: What does NVIDIA's CUDA programming model enable?\n\nA5: NVIDIA's CUDA programming model opened the parallel processing capabilities of GPUs for general purpose computing.",
                    "relevance_score": 0.92,
                    "rank": 1,
                    "metadata": {}
                }
            ],
        }

        print("=" * 80)
        print("üîç RAG Evaluator - Example Evaluation")
        print("=" * 80)

        evaluation = evaluator.evaluate(**sample_response)

        print(f"\nüìä Overall Score: {evaluation['overall_score']}/10")
        print(f"\nüìã Breakdown:")
        for criterion, score in evaluation["scores"].items():
            weight = evaluator.criteria[criterion]["weight"]
            print(f"  - {criterion.capitalize()}: {score}/10 (weight: {weight*100}%)")

        print(f"\nüí≠ Reasoning:\n{evaluation['reasoning']}")

        print(f"\n‚úÖ Strengths:")
        for strength in evaluation["strengths"]:
            print(f"  ‚Ä¢ {strength}")

        print(f"\n‚ö†Ô∏è  Weaknesses:")
        for weakness in evaluation["weaknesses"]:
            print(f"  ‚Ä¢ {weakness}")


if __name__ == "__main__":
    main()
